# LSTM-iTransformer Model Toolkit

## Project Overview

This project implements a hybrid model combining LSTM and iTransformer for time series forecasting and other tasks. iTransformer is an efficient variant of the Transformer model (paper reference: [https://arxiv.org/abs/2310.06625](https://arxiv.org/abs/2310.06625)), and this project integrates it with LSTM to enhance time series prediction performance.

## Key Features

- Supports long and short-term time series forecasting
- Provides data preprocessing tools
- Includes model training and evaluation framework
- Supports anomaly detection and data imputation tasks
- Provides comment removal tools for code streamlining and analysis

## Code Structure


lstm+iTransfomer/
├── models/                 # Model Definitions
│   └── iTransformer.py     # LSTM-iTransformer hybrid model implementation
├── utils/                  # Utility Functions
│   ├── remove\_comments\_from\_file.py  # Comment removal tool
│   ├── calculate\_metrics.py          # Evaluation metrics calculation
│   ├── data\_process.py               # Data preprocessing
│   ├── masking.py                    # Data masking
│   ├── timefeatures.py               # Time feature processing
│   └── tools.py                      # General utility functions
├── data/                   # Data-related tools
│   └── time\_column\_generation.py    # Time series generation tool
└── LSTM+itransformer.py    # Main program


## Model Description

The LSTM-iTransformer model combines LSTM’s ability to process sequential data with the Transformer’s attention mechanism. The main structure includes:

1. **Embedding Layer**: Converts input data into high-dimensional feature representations
2. **LSTM Layer**: Captures temporal features within the sequence data
3. **Transformer Encoder**: Uses self-attention mechanisms to capture long-range dependencies
4. **Projection Layer**: Outputs corresponding results based on different tasks (forecasting, imputation, anomaly detection, etc.)

The model supports multiple task types and can be switched via configuration parameters.

## Usage

### Environment Setup

- Python 3.10+
- PyTorch
- Pandas
- NumPy

### Basic Usage

1. Prepare time series data
2. Configure model parameters
3. Initialize and train the model
4. Perform forecasting or other tasks

### Comment Removal Tool

The project provides two versions of the comment removal tool to clean comments from code:

1. Using the `tokenize` module (more accurate for handling Python syntax):

```python
from utils.remove_comments_from_file import remove_comments_from_file
remove_comments_from_file("input.py", "output.py")

2. Using regular expressions (faster processing):

```python
from utils.remove_comments_from_file import remove_comments_from_file
remove_comments_from_file("input.py", "output.py")
```

### Time Series Generation

You can use the `data/time_column_generation.py` tool to generate time series data:

```bash
python data/time_column_generation.py
```

## Task Types

The model supports the following task types, which can be specified via the `task_name` configuration parameter:

* Long-Term Forecasting (`long_term_forecast`)
* Short-Term Forecasting (`short_term_forecast`)
* Data Imputation (`imputation`)
* Anomaly Detection (`anomaly_detection`)
* Classification (`classification`)

## License

This project is licensed under the MIT License. See the LICENSE file for details.

## References

* iTransformer: Inverted Transformers Are Effective for Time Series Forecasting, [https://arxiv.org/abs/2310.06625](https://arxiv.org/abs/2310.06625)

```

---

# LSTM-Transformer Hybrid Model Project

## Project Overview

This project implements a hybrid model that integrates LSTM (Long Short-Term Memory) and Transformer architectures for time series analysis tasks. By combining LSTM's strength in capturing temporal sequential patterns with Transformer's advantage in modeling long-range dependencies through self-attention mechanisms, the model achieves robust performance in various time series scenarios.

## Key Features

- Supports both short-term and long-term time series forecasting
- Includes complete data preprocessing pipelines (normalization, time feature engineering)
- Provides end-to-end model training and validation framework with early stopping
- Implements multiple evaluation metrics (R², MSE, RMSE, MAE, MAPE)
- Supports GPU acceleration for efficient training
- Configurable model parameters for task adaptation

## Code Structure

```

LSTM+Transformer/
├── LSTM-Transformer.py         # Main program with model definition and training logic
├── models/                    # Model architectures
│   ├── iTransformer.py         # LSTM-iTransformer hybrid model implementation
│   └── Transformer.py          # Vanilla Transformer implementation
├── layers/                    # Neural network components
│   ├── Transformer\_EncDec.py   # Transformer encoder/decoder layers
│   ├── SelfAttention\_Family.py # Attention mechanisms
│   ├── Embed.py                # Embedding layers (positional, temporal, etc.)
│   ├── ETSformer\_EncDec.py    # ETS-based transformer components
│   └── Autoformer\_EncDec.py   # Autoformer components
├── utils/                     # Utility functions
│   ├── tools.py               # Data transformation utilities
│   └── timefeatures.py        # Time feature extraction
├── data\_provider/             # Data loading and processing
│   └── data\_loader.py         # Dataset classes for different time series formats
└── data/                      # Data storage
└── \[time series data files]

````

## Model Description

The LSTM-Transformer hybrid model combines LSTM's sequential processing capability with Transformer's attention mechanism. The core structure includes:

1. **Embedding Layer**: Converts input time series data into high-dimensional feature representations using token embedding, positional embedding, and temporal embedding.
2. **LSTM Layer**: Processes sequential data to capture short-term temporal dependencies and local patterns.
3. **Transformer Encoder-Decoder**: Uses multi-head self-attention to model long-range dependencies between time steps.
4. **Projection Layer**: Maps the high-dimensional features to the target output space based on specific tasks.

The model's forward process involves:
- Input normalization and time feature encoding
- Feature extraction through LSTM
- Feature enhancement via Transformer's attention mechanism
- Prediction generation through the decoder and projection layer

## Usage

### Environment Setup

- Python 3.10+
- PyTorch (with CUDA support recommended)
- Pandas & NumPy
- Scikit-learn
- Matplotlib

### Basic Workflow

#### Data Preparation:

- Prepare time series data in CSV format with a date column and feature columns.
- Ensure the target variable is properly specified.

#### Parameter Configuration:
- Set sequence length (window), prediction length (length_size).
- Configure model hyperparameters (hidden dimension, number of layers, attention heads, etc.).
- Adjust training parameters (batch size, learning rate, epochs).

#### Model Training:

- Initialize the model with specified configurations.
- Use `model_train` or `model_train_val` function for training with optional validation.
- Monitor training/validation loss for performance tracking.

#### Prediction & Evaluation:
- Generate predictions on test data.
- Use `cal_eval` function to compute evaluation metrics.
- Analyze results with visualization tools.

### Example Code Snippets

#### Data Loading and Preprocessing:

```python
# Load data
df = pd.read_csv('path/to/your/data.csv', encoding='gbk')

# Extract time features
df_stamp = df[['date']].copy()
df_stamp['date'] = pd.to_datetime(df_stamp['date'])
data_stamp = time_features(df_stamp, timeenc=1, freq='T')

# Normalize data
scaler = MinMaxScaler()
data_normalized = scaler.fit_transform(df[feature_columns].values)
````

#### Model Initialization:

```python
config = Config()
config.seq_len = 10
config.pred_len = 1
config.d_model = 40
config.n_heads = 8

model = TransformerModel(config, device).to(device)
criterion = nn.MSELoss().to(device)
optimizer = optim.Adam(model.parameters(), lr=0.0002)
```

#### Training:

```python
trained_model, train_loss, final_epoch = model_train(
    net=model,
    train_loader=train_loader,
    length_size=config.pred_len,
    target_dim=1,
    optimizer=optimizer,
    criterion=criterion,
    num_epochs=config.num_epochs,
    device=device,
    print_train=True
)
```

## Supported Task Types

The model primarily focuses on time series forecasting tasks, with support for:

* Short-term forecasting (`short_term_forecast`)
* Long-term forecasting (`long_term_forecast`)

### Evaluation Metrics

The project provides comprehensive evaluation metrics through the `cal_eval` function:

* R² Score (Coefficient of Determination)
* MSE (Mean Squared Error)
* RMSE (Root Mean Squared Error)
* MAE (Mean Absolute Error)
* MAPE (Mean Absolute Percentage Error)

## References

* Vaswani, A., et al. (2017). Attention is all you need. Advances in neural information processing systems.
* Hochreiter, S., & Schmidhuber, J. (1997). Long short-term memory. Neural computation.

```

This markdown is now well-organized, easy to read, and properly indented.
```
